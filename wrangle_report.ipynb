{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report #weratedogs\n",
    "## Introduction\n",
    "---\n",
    "This report describes the wrangling proces that led to the act_report. The main data source is twitter-archive-enhanced.csv file which is a comma sepparated file created by Weratedogs for Udacity. Weratedogs created this file by downloading their twitter archive containing basic tweet data for all 5000+ of their tweets as they stood on August 1, 2017.\n",
    "The data wrangling process consists of\n",
    "* Data gathering\n",
    "* Assessing data\n",
    "* cleaning data\n",
    "Naturally this is an itterative process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data gathering\n",
    "---\n",
    "Data was gathered from three places as described below.\n",
    "* WeRateDogs Twitter archive. This archive contains basic tweet data for all 5000+ of their tweets.\n",
    "* Twitter api. We use the Twitter api to extract some extra data not present in the archive.\n",
    "* Tweet image prediction. Udacity used a neural network able to classify breeds of dogs to classify the images from weratedogs archive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering data from the Weratedogs Twitter archive\n",
    "WeRateDogs downloaded their Twitter archive and sent it to Udacity via email. This archive contains basic tweet data (tweet ID, timestamp, text, etc.) for all 5000+ of their tweets as they stood on August 1, 2017. This archive was downloaded manually from the following link [twitter-archive-enhanced.csv](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv). This file was read into a dataframe (df_archive) using panda's read_csv function.\n",
    "\n",
    "A description of what the column names stand for can be found [here.](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet image predictions\n",
    "The tweet image predictions, what breed of dog is present in each tweet according to a neural network is saved in a tab separated file (image_predictions.tsv). This file is hosted on Udacity's servers at the following url: [image_predictions.tsv](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv). This file was downloaded programmatically using the Requests library. Finnally the file was read into a dataframe (df_image_predictions) using panda's read_csv function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter API\n",
    "Extra information was received from the Twitter API using the tweepy library. In order to use this library I had to install it first on my system. For this I used conda.\n",
    "```\n",
    "conda install -c conda-forge tweepy\n",
    "```\n",
    "To retrieve data from the Twitter API I needed the Tweet id's. This list was retrived from the weratedogs archive and from the tweet image prediction. Naturally duplicates were removed.\n",
    "The results were saved into another dataframe.\n",
    "Finally the data was saved to the 'tweet_json.txt' file using panda's to_csv function.\n",
    "\n",
    "A description of what the column names stand for can be found [here.](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assesing data\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in assesing the data was a visual inspection. This was done by simply displaying the three dataframes ,go through the data visually, and writting down what was not correct.\n",
    "\n",
    "The second step was assesing the dataframes programatically. This was done by using the following Pandas functions.\n",
    "* DataFrame.info -- returns a concise summary of a DataFrame.\n",
    "* Series.nunique -- returns number of unique elements in the object.\n",
    "* Series.value_counts -- returns object containing counts of unique values.\n",
    "\n",
    "The issues found are listed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness issues (structual issues)\n",
    "* To much information in df_image_predictions dataframe, (tweet_id and jpg_url is what we need)\n",
    "* Various stages of dogs (doggo, floofer, pupper, puppo) are in four columns.\n",
    "* All dataframes should be merged into one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dirtiness issues (quality)\n",
    "* wrong datatypes for seeral columns\n",
    "* Tweets without images\n",
    "* Dataset contains retweets\n",
    "* Incorrect dog names\n",
    "* Sources difficult to read\n",
    "* Some tweet_ids have the same jpg_url\n",
    "* Some pictures are not of dogs\n",
    "* Some pictures are flagged as not a dog but are in fact of a dog.\n",
    "* Fractions in the text are mistaken for rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps were taken to clean the data.\n",
    "1. In the 'source' column I removed the html tags using a regular expression. The type of the column was changed into a category.\n",
    "2. The 'doggo', 'floofer', 'pupper', and 'puppo' columns were merged into a 'dog_stage' column.\n",
    "3. The three dataframes were merged into one.\n",
    "4. Only original tweets were kept. That means that rows where retweeted_status_user_id and in_reply_to_user_id are not null wee removed. \n",
    "5. After this all columns related to retweeting and replying were removed.\n",
    "6. Records without images were removed.\n",
    "7. I only wanted to keep predictions with the greatest confidence so all columns related to p2 and p3 were removed.\n",
    "8. Multiple columns were changed to the correct datatype.\n",
    "9. ratings that were not really ratings were fixed\n",
    "10. Tweets without rating were deleted\n",
    "11. All ratings were converted to be out of 10\n",
    "12. Invalid dognames were fixed as far as possible.\n",
    "\n",
    "Note: many tweets did not show a dog name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the cleaned data was stored in 'twitter_archive_cleaned_and_shiny.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
